<!DOCTYPE html>
<html>
<title>Unsupervised Learning of Optical Flow with Deep Feature Similarity</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
body {
	background-color: black;
	color: white;
}

a {
	color: white;
}

h1, h2, h3, h4, h5, h6 {
	text-align: center;
}

a:hover {
	text-decoration: underline;
}

article, header, footer {
	max-width: 700px;
	margin-left: auto;
	margin-right: auto;
	
	border-top: 1px solid grey;
}


.author {
	text-decoration: None;
}

.isDisabled {
  color: currentColor;
  cursor: not-allowed;
  opacity: 0.5;
  text-decoration: none;
}
.resp-container {
    position: relative;
    overflow: hidden;
    padding-top: 56.25%;
}
.resp-iframe {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    border: 0;
}
</style>
<body>

<header id="home">
  <div class="w3-display-middle w3-center">
    <h1>Unsupervised Learning of Optical Flow with Deep Feature Similarity</h1>
	<h2>
		<a class="author" href="https://iwbn.github.io/">Woobin Im</a>, 
		<a class="author" href="https://sites.google.com/view/tkkim/">Tae-Kyun Kim</a>, and 
		<a class="author" href="https://sgvr.kaist.ac.kr/~sungeui/">Sung-Eui Yoon</a>
	</h2>
	<h3>
	European Conference on Computer Vision (ECCV) 2020
	</h3>
	<h4>
	<a href="https://sgvr.kaist.ac.kr/wp-content/uploads/2020/07/4544.pdf" target="_blank">[paper]</a>
	<a href="https://sgvr.kaist.ac.kr/wp-content/uploads/2020/09/ECCV2020_presentation_release.pptx" target="_blank">[slides]</a>
	<span class="isDisabled" href="" disabled>[github (available soon!)]</span>
	</h4>
  </div>
</header>

<article>
<h2>Short introduction video</h2>
<div class="resp-container">
<iframe class="resp-iframe" src="https://www.youtube.com/embed/kVNDyuBO4s0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<h2>Overview</h2>
<img src="figure.PNG" style="width:100%" >
This shows the overview of our method, which is end-to-end trainable for
both optical 
ow and self-supervised deep features

<div id="abstract">
	<h2>Abstract</h2>
    <p style="text-align:justify;">Deep unsupervised learning for optical flow has been proposed, where the loss measures image similarity 
	with the warping function parameterized by estimated flow. The census transform, instead of image pixel values, 
	is often used for the image similarity. In this work, rather than the handcrafted features i.e. census or pixel values, 
	we propose to use deep self-supervised features with a novel similarity measure, which fuses multi-layer similarities. 
	With the fused similarity, our network better learns flow by minimizing our proposed feature separation loss. 
	The proposed method is a polarizing scheme, resulting in a more discriminative similarity map. In the process, 
	the features are also updated to get high similarity for matching pairs and low for uncertain pairs, given estimated flow. 
	We evaluate our method on FlyingChairs, MPI Sintel, and KITTI benchmarks. In quantitative and qualitative comparisons, 
	our method effectively improves the state-of-the-art techniques.</p>
</div>
</article>
<footer style="text-align:right;">
<a href="https://kaist.ac.kr/"><img height="40" src="kaist_logo_trans(white).png"></a>
</footer>
<script>
</script>
</body>
</html>
